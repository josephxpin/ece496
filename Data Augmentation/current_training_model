{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"current_training_model","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"1WjiiaYleqXZ"},"source":["import matplotlib.pyplot as plt # plotting library\n","import numpy as np # this module is useful to work with numerical arrays\n","import pandas as pd \n","import random \n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torch.utils.data import DataLoader,random_split\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import tensorflow as tf\n","\n","data_dir = 'dataset'\n","\n","train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n","test_dataset  = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n","\n","train_transform = transforms.Compose([\n","transforms.ToTensor(),\n","])\n","\n","test_transform = transforms.Compose([\n","transforms.ToTensor(),\n","])\n","\n","train_dataset.transform = train_transform\n","test_dataset.transform = test_transform\n","\n","m=len(train_dataset)\n","\n","train_data, val_data = random_split(train_dataset, [int(m-m*0.2), int(m*0.2)])\n","batch_size=256\n","\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n","valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[0][0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88RXqFcuUGc4","executionInfo":{"status":"ok","timestamp":1648588330608,"user_tz":240,"elapsed":290,"user":{"displayName":"Mohamed Ahmad","userId":"13173598713220151846"}},"outputId":"87f0272a-2455-4fba-f3a6-3f0dc0deb0ad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 28, 28])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[""],"metadata":{"id":"hTU6h0OL1vEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPCHXmgae3EZ"},"source":["class Encoder(nn.Module):\n","    \n","    def __init__(self, encoded_space_dim,fc2_input_dim):\n","        super().__init__()\n","        \n","        ### Convolutional section\n","        self.encoder_cnn = nn.Sequential(\n","            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(True),\n","            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n","            nn.ReLU(True)\n","        )\n","        \n","        ### Flatten layer\n","        self.flatten = nn.Flatten(start_dim=1)\n","### Linear section\n","        self.encoder_lin = nn.Sequential(\n","            nn.Linear(3 * 3 * 32, 128),\n","            nn.ReLU(True),\n","            nn.Linear(128, encoded_space_dim)\n","        )\n","        \n","    def forward(self, x):\n","        x = self.encoder_cnn(x)\n","        x = self.flatten(x)\n","        x = self.encoder_lin(x)\n","        return x\n","class Decoder(nn.Module):\n","    \n","    def __init__(self, encoded_space_dim,fc2_input_dim):\n","        super().__init__()\n","        self.decoder_lin = nn.Sequential(\n","            nn.Linear(encoded_space_dim, 128),\n","            nn.ReLU(True),\n","            nn.Linear(128, 3 * 3 * 32),\n","            nn.ReLU(True)\n","        )\n","\n","        self.unflatten = nn.Unflatten(dim=1, \n","        unflattened_size=(32, 3, 3))\n","\n","        self.decoder_conv = nn.Sequential(\n","            nn.ConvTranspose2d(32, 16, 3, \n","            stride=2, output_padding=0),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(16, 8, 3, stride=2, \n","            padding=1, output_padding=1),\n","            nn.BatchNorm2d(8),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(8, 1, 3, stride=2, \n","            padding=1, output_padding=1)\n","        )\n","        \n","    def forward(self, x):\n","        x = self.decoder_lin(x)\n","        x = self.unflatten(x)\n","        x = self.decoder_conv(x)\n","        x = torch.sigmoid(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lr_predict(w, b, x):\n","    # y_hat = sigmoid (wt x + b)\n","    \n","    # w is (D, )\n","    # x is (N, D), N vectors not just one\n","\n","    return  sigmoid(x @ w + b)\n","\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))"],"metadata":{"id":"YszMQCXV8pEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def buildGraph(alpha = 1e-3):\n","   # Weight and bias tensors\n","   graph = tf.Graph()\n","   tf.set_random_seed(1000)\n","\n","   w = tf.Variable(tf.random.truncated_normal(shape=[28*28, 1],stddev=0.5))\n","   b = tf.Variable(tf.random.truncated_normal(shape=[1],stddev=0.5))\n","   # Placeholders for data, labels and reg \n","   # SHould we add a label to hi-fi and lo-fi\n","   data = tf.placeholder(dtype=tf.float32, shape = [None, 28*28], name = 'data')\n","   labels = tf.placeholder(dtype=tf.float32, shape = [None, 1], name = 'labels')\n","   reg = tf.placeholder(dtype=tf.float32, name = 'reg')\n","   # Predicted labels\n","   z = tf.matmul(data, w) + b\n","   y_hat = tf.sigmoid(z)\n","   # Loss tensor\n","   loss =  tf.losses.sigmoid_cross_entropy(multi_class_labels = labels, logits = y_hat) + tf.multiply(reg/2, tf.reduce_sum(tf.square(w)))\n","   # Optimizer\n","   optimizer = tf.train.AdamOptimizer(learning_rate = 0.001, name = 'optimizer').minimize(loss)\n","\n","   return w, b, y_hat, labels, loss, optimizer, reg"],"metadata":{"id":"AOP2xV3D-wUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# '''with tf.Session() as session:\n","#     tf.global_variables_initializer().run()\n","    \n","#     # ------------ 1. Training Autoencoders - Unsupervised Learning ----------- #\n","#     for epoch in range(training_epochs):\n","#         epoch_costs = np.empty(0)\n","#         for b in range(total_batches):\n","#             offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n","#             batch_x = train_x[offset:(offset + batch_size), :]\n","#             _, c = session.run([optimizer, loss],feed_dict={data: batch_x, labels:, reg:})\n","#             epoch_costs = np.append(epoch_costs,c)"],"metadata":{"id":"t8ozuytI88eF"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMbSVqowe81B","executionInfo":{"status":"ok","timestamp":1648588420105,"user_tz":240,"elapsed":6,"user":{"displayName":"Mohamed Ahmad","userId":"13173598713220151846"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7cf1ac04-e992-4b3e-d8ae-7d46632c221f"},"source":["### Define the loss function\n","loss_fn = torch.nn.MSELoss()\n","\n","### Define an optimizer (both for the encoder and the decoder!)\n","lr= 0.001\n","\n","### Set the random seed for reproducible results\n","torch.manual_seed(0)\n","\n","### Initialize the two networks\n","d = 4\n","\n","#model = Autoencoder(encoded_space_dim=encoded_space_dim)\n","encoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\n","decoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)\n","params_to_optimize = [\n","    {'params': encoder.parameters()},\n","    {'params': decoder.parameters()}\n","]\n","\n","optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n","\n","# Check if the GPU is available\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f'Selected device: {device}')\n","\n","# Move both the encoder and the decoder to the selected device\n","encoder.to(device)\n","decoder.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selected device: cuda\n"]},{"output_type":"execute_result","data":{"text/plain":["Decoder(\n","  (decoder_lin): Sequential(\n","    (0): Linear(in_features=4, out_features=128, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Linear(in_features=128, out_features=288, bias=True)\n","    (3): ReLU(inplace=True)\n","  )\n","  (unflatten): Unflatten(dim=1, unflattened_size=(32, 3, 3))\n","  (decoder_conv): Sequential(\n","    (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2))\n","    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","  )\n",")"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["!pip install pytorch-lightning"],"metadata":{"id":"77_yTsdM87f5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648665284929,"user_tz":240,"elapsed":31797,"user":{"displayName":"Mohamed Ahmad","userId":"13173598713220151846"}},"outputId":"e6327d17-e492-48cb-e086-5d1e11335780"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-1.6.0-py3-none-any.whl (582 kB)\n","\u001b[K     |████████████████████████████████| 582 kB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.8.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.63.0)\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n","\u001b[K     |████████████████████████████████| 134 kB 25.2 MB/s \n","\u001b[?25hCollecting typing-extensions>=4.0.0\n","  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.21.5)\n","Collecting pyDeprecate<0.4.0,>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n","Collecting PyYAML>=5.4\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 32.9 MB/s \n","\u001b[?25hCollecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.7.3-py3-none-any.whl (398 kB)\n","\u001b[K     |████████████████████████████████| 398 kB 34.8 MB/s \n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 22.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.7)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.44.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.0 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 32.6 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 35.9 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n","Installing collected packages: typing-extensions, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, pytorch-lightning\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 3.10.0.2\n","    Uninstalling typing-extensions-3.10.0.2:\n","      Successfully uninstalled typing-extensions-3.10.0.2\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n","Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.0 torchmetrics-0.7.3 typing-extensions-4.1.1 yarl-1.7.2\n"]}]},{"cell_type":"code","metadata":{"id":"XRYYYT04fBE5"},"source":["\n","def add_noise(inputs,noise_factor=0.3):\n","     noisy = inputs+torch.randn_like(inputs) * noise_factor\n","     noisy = torch.clip(noisy,0.,1.)\n","     return noisy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2ofFDb_fIGY"},"source":["### Training function\n","def train_epoch_den(encoder, decoder, device, dataloader, loss_fn, optimizer,noise_factor=0.3):\n","    # Set train mode for both the encoder and the decoder\n","    encoder.train()\n","    decoder.train()\n","    train_loss = []\n","    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n","    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n","        # Move tensor to the proper device\n","        image_noisy = add_noise(image_batch,noise_factor)\n","        image_noisy = image_noisy.to(device)    \n","        # Encode data\n","        encoded_data = encoder(image_noisy)\n","        # Decode data\n","        decoded_data = decoder(encoded_data)\n","        # Evaluate loss\n","        loss = loss_fn(decoded_data, image_noisy)\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # Print batch loss\n","\n","        train_loss.append(loss.detach().cpu().numpy())\n","    return np.mean(train_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kto7kWWafLz0"},"source":["### Testing function\n","def test_epoch_den(encoder, decoder, device, dataloader, loss_fn,noise_factor=0.3):\n","    # Set evaluation mode for encoder and decoder\n","    encoder.eval()\n","    decoder.eval()\n","    with torch.no_grad(): # No need to track the gradients\n","        # Define the lists to store the outputs for each batch\n","        conc_out = []\n","        conc_label = []\n","        for image_batch, _ in dataloader:\n","            # Move tensor to the proper device\n","            image_noisy = add_noise(image_batch,noise_factor)\n","            image_noisy = image_noisy.to(device)\n","            # Encode data\n","            print(image_noisy.shape)\n","            print(\"@DEDEDEDEDEDEDEDEDE\")\n","            return\n","            encoded_data = encoder(image_noisy)\n","            # Decode data\n","            decoded_data = decoder(encoded_data)\n","            # Append the network output and the original image to the lists\n","            conc_out.append(decoded_data.cpu())\n","            conc_label.append(image_batch.cpu())\n","        # Create a single tensor with all the values in the lists\n","        conc_out = torch.cat(conc_out)\n","        conc_label = torch.cat(conc_label) \n","        # Evaluate global loss\n","        val_loss = loss_fn(conc_out, conc_label)\n","    return val_loss.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmQQKzLufTag"},"source":["\n","def plot_ae_outputs_den(encoder,decoder,n=5,noise_factor=0.3):\n","    plt.figure(figsize=(10,4.5))\n","    for i in range(n):\n","\n","      ax = plt.subplot(3,n,i+1)\n","      img = test_dataset[i][0].unsqueeze(0)\n","      image_noisy = add_noise(img,noise_factor)     \n","      image_noisy = image_noisy.to(device)\n","\n","      encoder.eval()\n","      decoder.eval()\n","\n","      with torch.no_grad():\n","         rec_img  = decoder(encoder(image_noisy))\n","\n","      plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n","      ax.get_xaxis().set_visible(False)\n","      ax.get_yaxis().set_visible(False)  \n","      if i == n//2:\n","        ax.set_title('High fidelity images')\n","      ax = plt.subplot(3, n, i + 1 + n)\n","      plt.imshow(image_noisy.cpu().squeeze().numpy(), cmap='gist_gray')\n","      ax.get_xaxis().set_visible(False)\n","      ax.get_yaxis().set_visible(False)  \n","      if i == n//2:\n","        ax.set_title('Low fidelity images')\n","\n","      ax = plt.subplot(3, n, i + 1 + n + n)\n","      plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n","      ax.get_xaxis().set_visible(False)\n","      ax.get_yaxis().set_visible(False)  \n","      if i == n//2:\n","         ax.set_title('Reconstructed images')\n","    plt.subplots_adjust(left=0.1,\n","                    bottom=0.1, \n","                    right=0.7, \n","                    top=0.9, \n","                    wspace=0.3, \n","                    hspace=0.3)     \n","    plt.show()   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TI0YZaZsfcPH","executionInfo":{"status":"error","timestamp":1648588444017,"user_tz":240,"elapsed":4680,"user":{"displayName":"Mohamed Ahmad","userId":"13173598713220151846"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c89581e3-e95a-44fb-a3b6-902585b25467"},"source":["### Training cycle\n","noise_factor = 0.45\n","num_epochs = 50\n","history_da={'train_loss':[],'val_loss':[]}\n","train_loss = []\n","val_loss = []\n","for epoch in range(num_epochs):\n","    print('EPOCH %d/%d' % (epoch + 1, num_epochs))\n","    ### Training (use the training function)\n","    train_loss.append(train_epoch_den(\n","        encoder=encoder, \n","        decoder=decoder, \n","        device=device, \n","        dataloader=train_loader, \n","        loss_fn=loss_fn, \n","        optimizer=optim,noise_factor=noise_factor))\n","    ### Validation  (use the testing function)\n","    val_loss.append(test_epoch_den(\n","        encoder=encoder, \n","        decoder=decoder, \n","        device=device, \n","        dataloader=valid_loader, \n","        loss_fn=loss_fn,noise_factor=noise_factor)) \n","        \n","    # Print Validationloss\n","    history_da['train_loss'].append(train_loss)\n","    history_da['val_loss'].append(val_loss)\n","    #print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n","    #plot_ae_outputs_den(encoder,decoder,noise_factor=noise_factor)\n","\n","plt.figure(figsize=(20,8))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Loss vs Epoch\") \n","plt.plot(range(0, len(train_loss)),train_loss,label='trainloss') \n","plt.plot(range(0, len(val_loss)),val_loss,label='validloss') "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([128, 288])\n","torch.Size([256, 1, 28, 28])\n","@DEDEDEDEDEDEDEDEDE\n","EPOCH 2/50\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n","torch.Size([256, 288])\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-d753def1d8ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         optimizer=optim,noise_factor=noise_factor))\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m### Validation  (use the testing function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     val_loss.append(test_epoch_den(\n","\u001b[0;32m<ipython-input-35-ead6f42db59e>\u001b[0m in \u001b[0;36mtrain_epoch_den\u001b[0;34m(encoder, decoder, device, dataloader, loss_fn, optimizer, noise_factor)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Iterate the dataloader (we do not need the label values, this is unsupervised learning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# with \"_\" we just ignore the labels (the second element of the dataloader tuple)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Move tensor to the proper device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mimage_noisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}